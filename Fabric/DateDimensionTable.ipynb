{"cells":[{"cell_type":"markdown","source":["### Creating a Date Dimension Table in Microsoft Fabric Lakehouse\n","By Romain Casteres\n"," \n","Blog Article : \n","- EN : https://www.linkedin.com/pulse/generating-date-dimension-table-direct-lake-model-fabric-casteres-fmjue\n","- FR : https://pulsweb.fr/dimension-date-direct-lake/"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9a527033-01e9-496e-bc8f-b89f1671533e"},{"cell_type":"code","source":["import pandas as pd\n","import holidays     #https://pypi.org/project/holidays/\n","\n","def DimDate(start_date, end_date):\n","    from pandas.tseries.offsets import MonthEnd, QuarterEnd\n","    dfdate = pd.DataFrame({\"Date\": pd.date_range(start=f'{start_date}', end=f'{end_date}', freq='D')})\n","    def get_end_of_month(date):\n","        if date.is_month_end == True:\n","            return date\n","        else:\n","            return date + MonthEnd(1)\n","    def get_end_of_quarter(date):\n","        if date.is_quarter_end == True:\n","            return date\n","        else:\n","            return date + QuarterEnd(1)\n","    dfdate[\"Day\"] = dfdate.Date.dt.day\n","    dfdate[\"Week\"] = dfdate.Date.dt.weekday\n","    dfdate[\"Month\"] = dfdate.Date.dt.month\n","    dfdate[\"MonthName\"] = dfdate.Date.dt.month_name()\n","    dfdate[\"Quarter\"] = dfdate.Date.dt.quarter\n","    dfdate[\"Year\"] = dfdate.Date.dt.year\n","    dfdate[\"FiscalYear\"] = dfdate['Date'].dt.to_period('A-JUN')\n","    dfdate['EndOfMonth'] = dfdate['Date'].apply(get_end_of_month)\n","    dfdate['EOM'] = dfdate['Date'].dt.is_month_end\n","    dfdate['EndOfQuarter'] = dfdate['Date'].apply(get_end_of_quarter)\n","    dfdate['EOQ'] = dfdate['Date'].dt.is_quarter_end\n","    return dfdate\n","\n","def DimDateWorkday(start_date, end_date):\n","    dfdate = DimDate(start_date, end_date)\n","    dfdate['Workday'] = True\n","    for index, row in dfdate.iterrows():\n","        if row['Day'] in ['Saturday', 'Sunday']:\n","            dfdate.loc[index, 'Workday'] = False\n","        date = row['Date'].strftime(\"%Y-%m-%d\")\n","        if date in holidays.France():\n","            dfdate.loc[index, 'Workday'] = False\n","    return dfdate\n","\n","#DimDate('2024-01-01', '2025-12-31')\n","#DimDateWorkday('2024-01-01', '2025-12-31')\n","df_DimDate = spark.createDataFrame(DimDateWorkday('2024-01-01', '2025-12-31'))\n","df_DimDate.write.mode(\"overwrite\").format(\"delta\").saveAsTable('calandar')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"livy_statement_state":"available","session_id":"b540e040-6a41-4869-b26e-86b14b64c6d9","state":"finished","normalized_state":"finished","queued_time":"2024-08-01T15:19:41.4597319Z","session_start_time":"2024-08-01T15:19:41.8020996Z","execution_start_time":"2024-08-01T15:22:52.2112205Z","execution_finish_time":"2024-08-01T15:23:27.2880723Z","parent_msg_id":"b5141831-9352-4c86-89ab-85e2dc32b7da"},"text/plain":"StatementMeta(, b540e040-6a41-4869-b26e-86b14b64c6d9, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:428: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n  Unsupported type in conversion from Arrow: extension<pandas.period<ArrowPeriodType>>\nAttempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":true}},"id":"5cb2ec76-ec9c-4525-b915-6fbdcbda7fc4"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"587ff68f-bd64-4c3d-97e8-ec345ba966c9","default_lakehouse_name":"BPA_Lakehouse","default_lakehouse_workspace_id":"12bf1bca-ee95-4e8f-928b-d378aba54a7d"},"environment":{"environmentId":"d1be124a-478c-4883-9ccc-f07286b94a0f","workspaceId":"12bf1bca-ee95-4e8f-928b-d378aba54a7d"}}},"nbformat":4,"nbformat_minor":5}